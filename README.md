```markdown
# ğŸ§  RAG-Based Complaint Chatbot

A Retrieval-Augmented Generation (RAG) chatbot that files and retrieves customer complaints using a REST API and LLM-based reasoning with memory and knowledge integration.

---

## ğŸš€ Features

- ğŸ’¬ Chatbot interface to file and retrieve complaints
- ğŸ§  Uses RAG to answer general FAQs from a knowledge base
- ğŸ”§ Complaint creation and retrieval via API (`FastAPI`)
- ğŸ—ƒï¸ Complaints stored in SQLite
- ğŸ§  Local LLM support via [Ollama](https://ollama.com/)
- ğŸ“„ Knowledge base integration via FAISS
- ğŸ›ï¸ Clean and interactive UI with Streamlit

---

## ğŸ› ï¸ Project Structure

```

.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py            # FastAPI app with complaint endpoints
â”‚   â”œâ”€â”€ crud.py            # DB insert/retrieve logic
â”‚   â”œâ”€â”€ database.py        # SQLite setup
â”‚   â””â”€â”€ models.py          # Pydantic models & validation
â”œâ”€â”€ chatbot/
â”‚   â”œâ”€â”€ agent.py           # LangGraph-based agent logic
â”‚   â”œâ”€â”€ config.py          # OpenAI/HF config toggle
â”‚   â”œâ”€â”€ rag\_retriever.py   # FAISS retriever
â”‚   â””â”€â”€ tools.py           # Tools exposed to agent
â”œâ”€â”€ knowledge-base/
â”‚   â””â”€â”€ faq.txt            # Text knowledge base for RAG
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py             # Streamlit frontend
â”œâ”€â”€ complaints.db          # (Ignored by Git, autogenerated)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ“¦ Setup Instructions

### 1. Clone & Install

```bash
git clone https://github.com/your-repo/rag-chatbot.git
cd rag-chatbot
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows
pip install -r requirements.txt
````

---

### 2. Start Backend API

```bash
uvicorn backend.main:app --reload --port 8000
```

Runs the complaint registration & retrieval API on `http://localhost:8000`.

---

### 3. Start the UI (Chatbot Interface)

```bash
streamlit run ui/app.py
```

This launches the chatbot frontend.

---

## ğŸ¤– LLM Setup with Ollama

Install [Ollama](https://ollama.com/) if you havenâ€™t:

```bash
# On Mac/Linux/Windows (via terminal or powershell)
curl -fsSL https://ollama.com/install.sh | sh
```

Pull the LLM model:

```bash
ollama pull llama3:8b
```

Or, for your current model:

```bash
ollama pull llama3.2:3b
```

Then start Ollama server (usually runs automatically in background):

```bash
ollama run llama3.2:3b
```

Make sure `config.py` uses:

```python
use_openai = False
HF_MODEL_NAME = "Qwen/Qwen3-8B"  # Only used if you're loading HF models manually
```

And your local LLM base URL is set in `agent.py` (default is `http://localhost:11434`).

---

## ğŸ’¡ Sample Interactions

```
User: I want to register a complaint.
Bot: I'd be happy to help. May I know your name?
...
Bot: âœ… Your complaint has been registered. ID: ABC123
```

```
User: Show my complaint details for ID ABC123.
Bot: Here are the details of your complaint...
```

---

## ğŸ“š Knowledge Base

Edit `knowledge-base/faq.txt` to change answers for general questions. The bot will use semantic search + RAG to answer based on this file.

---

## âœ… APIs Available

| Endpoint           | Method | Description              |
| ------------------ | ------ | ------------------------ |
| `/complaints`      | POST   | Register a new complaint |
| `/complaints/{id}` | GET    | Retrieve complaint by ID |

---

## ğŸ§ª Testing

You can test endpoints with tools like Postman or via cURL:

```bash
curl -X POST http://localhost:8000/complaints \
  -H "Content-Type: application/json" \
  -d '{"name": "Aadesh", "phone_number": "9876543210", "email": "aadesh@gmail.com", "complaint_details": "Wrong product received"}'
```

---

## ğŸ™‹ Support

Feel free to open issues or suggestions if you'd like to improve this bot or add more tools like:

* Escalation handling
* Live agent handoff
* Auto-email notifications

---

Happy Building! ğŸš€

```

---

Let me know if you want me to generate a `requirements.txt` or Streamlit `app.py` walkthrough next.
```
